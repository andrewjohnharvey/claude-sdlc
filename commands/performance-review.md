# Performance Audit

Analyzes the codebase for potential performance bottlenecks and inefficiencies. The `/performance-audit` command performs a **static analysis** of the entire project (no code execution or profiling) to identify issues such as inefficient loops, unnecessary recursion, N+1 query patterns, heavy memory usage, slow front-end rendering practices, and build misconfigurations. It produces a detailed report with findings and optimization suggestions.

## Instructions

1. **Initialize Performance Audit**
    - **No Arguments:** This command requires no arguments. If any argument is mistakenly provided, politely explain that none are needed and proceed to analyze the entire codebase.
    - **Set Scope:** Default to scanning the **entire repository** for performance issues. Use shell commands (e.g., **!**`find . -type f`) to gather a list of relevant source files. Exclude non-code or external dependency directories (e.g., `node_modules/`, `vendor/`, build artifacts) so the audit focuses only on project code.
    - **Prepare Context:** Ensure all source code files are accessible for analysis. If the project has any performance-related configuration or documentation (for example, architecture notes on optimization in `.claude-sdlc/architecture/`), open and reference those to understand known constraints or past optimizations. This provides context but is optional – the primary analysis will rely on scanning the code itself.
    - **Static Analysis Only:** Do **not** run the application, tests, or any profiling tools as part of this command. The audit should rely purely on static code inspection and heuristics. (For instance, do not attempt to measure execution time – infer potential slowness from code patterns.) Confirm that the environment is ready for read-only analysis and proceed.
2. **Perform Static Analysis (By Category)**  
    Examine the codebase for performance red flags, organizing the analysis into categories. For each category below, **identify any problematic code patterns** and note them for the report, along with why they are problematic and how to improve them:
    - **Algorithmic Complexity (Loops & Recursion):** Inspect all functions and methods for heavy or nested loops and recursive calls. Flag any **hot loops** that appear to run in O(n²) or worse (e.g. double nested loops over large data sets) or deep **recursion** that could be replaced with iteration or memoization. Consider whether these could be optimized – for example, using caching or more efficient algorithms. Note where a loop processes large collections in an obvious way that could become slow, and suggest improvements (such as breaking out of loops early, using hash sets for faster lookups, or reducing loop nesting by restructuring logic).
    - **Database Access (N+1 Queries & I/O):** Analyze code that interacts with databases or external I/O. Look for patterns like **N+1 database queries** – e.g., queries inside a loop that could be consolidated into a single query. Identify any database calls that **retrieve large result sets** where pagination or filtering would be more efficient. If the code constructs queries without appropriate indexing or conditions (scanning entire tables), mark this as a potential issue. Similarly, check file I/O patterns: reading very large files entirely into memory or performing many small I/O operations in succession. For each issue, explain why the approach is suboptimal (e.g., “multiple repetitive queries can slow down requests”) and suggest optimizations (use lazy loading, batch queries, add an index, stream file data, etc.).
    - **Memory Utilization:** Review the code for patterns that might consume excessive memory. This includes assembling large in-memory data structures, storing extensive lists or maps of data when not necessary, or loading entire datasets where streaming or incremental processing would suffice. Highlight any instances where memory use could become a bottleneck (for example, constructing a list of all users in memory). Provide suggestions such as using generators or iterative processing, releasing objects sooner, or employing caching strategies with size limits. Also flag any use of inefficient data structures (e.g., using a list for membership checks where a set or hash map would be faster and more memory-efficient).
    - **Frontend Rendering (if applicable):** If the project contains frontend code (web or mobile UI), check for performance antipatterns in the client-side. Look at JavaScript/HTML/CSS files or frontend framework code for things like **inefficient DOM updates** (e.g., manipulating the DOM in a tight loop, or not using requestAnimationFrame for animations), lack of virtualization for large lists, or unthrottled events (like scroll or resize handlers doing heavy work). Examine bundle configurations or asset usage: flag **very large bundle sizes** or unoptimized assets (huge images or videos loaded without compression or lazy loading). Note any cases where rendering might be slow (for example, excessive re-renders in a React app due to improper state usage). For each issue, explain the impact (e.g., “could cause jank in the UI or slow load times”) and suggest fixes (such as debouncing events, splitting bundles, using pagination or virtualization for lists, optimizing images, etc.).
    - **Build & Configuration:** Review build scripts and configuration files (e.g., build tools like Webpack, parcel, makefiles, CI/CD pipeline configs) for potential inefficiencies that could lead to **excessive build times**. Identify any misconfigurations such as always running full clean builds without caching, not enabling production optimizations (minification, tree shaking, compiler optimizations), or running unnecessary tasks in series that could be parallelized. If the project’s build or compilation flags are set to a slower mode (e.g., debug mode or verbose logging always on), point that out. Also, consider test or deployment scripts if they impact performance (for instance, a test suite that runs in every build step without need). For each finding, describe why it slows down the build or deployment and provide an improvement suggestion (like enabling caching, adjusting build settings, or restructuring the pipeline for efficiency).

Throughout this analysis, **focus on known performance best practices**. Where applicable, reference these best practices in your notes (e.g., “avoid computing in a loop what can be pre-computed,” “use pagination for large data sets,” “enable gzip compression for responses,” etc.). **Do not write any code or make changes** at this stage – only identify and record potential issues.

3. **Compile Performance Report**
    - **Create Report File:** Consolidate all identified performance issues into a Markdown report. Create a new file under `.claude-sdlc/performance/` to store the results. Name the file with a timestamp for uniqueness (for example: `.claude-sdlc/performance/<timestamp>-performance-audit.md`, such as `.claude-sdlc/performance/2025-07-12T16-05-30-performance-audit.md`). If the `performance/` directory does not exist, create it first (using **!**`mkdir -p .claude-sdlc/performance`)
    - **Report Structure:** Begin the report with a **high-level summary** of the findings. For instance, note how many potential issues were found and in which categories (e.g., “Scanned the codebase and found 5 potential performance concerns across database queries, loops, and memory usage. Overall, most code is efficient, but a few hotspots may impact scalability.”). If there are any positive observations (for example, certain modules are well-optimized), you may briefly mention them to give balance.
    - **Categorize Findings:** Organize the detailed findings by the categories analyzed. Use section subheadings (e.g., “**Loops & Algorithmic Complexity**”, “**Database Queries**”, “**Memory Usage**”, etc.) for clarity. Under each category, list each identified issue as a bullet point or numbered item.
        - Reference the **file and location** (file path, and line number or function name if available) where the issue occurs to help the developer locate it. For example: `` `src/utils/data_parser.py:45` – Uses an O(n²) loop to parse input; this could slow down processing of large files. **Suggestion:** Consider using a set for lookups to achieve O(n) performance. ``
        - Clearly **describe why the code could be a performance problem**. Keep the explanation concise and factual, e.g., “Constructs a list of all records in memory, which might exhaust memory on large datasets,” or “Re-renders the entire page on every click, which may degrade UI responsiveness.”
        - Provide one or more **specific optimization suggestions or best practices** for each issue. For instance, “Use lazy loading or pagination to handle large data sets,” or “Move the invariant calculation outside the loop to avoid repeated work.” The suggestions should be actionable and concrete, giving the developer a clear idea of how to improve the performance.
    - **No Code Changes:** Ensure the report is strictly observational. Do **not** modify any source code as part of this audit. The report should describe issues and recommendations for the developer to consider, not apply fixes. Write in a professional, helpful tone, making it clear these findings are potential improvements. After compiling all issues and suggestions, proofread the report for clarity and save the file to disk.
        
4. **Summarize and Guide**
    - **Chat Summary:** After saving the performance report, output a concise summary in the chat. Start by confirming that the audit is complete and highlight the number of issues found. For example, **“✅ Performance audit complete. Identified 5 potential performance issues across the codebase.”** Mention that a detailed report has been saved and include the path to the report file (e.g., “See `.claude-sdlc/performance/2025-07-12T16-05-30-performance-audit.md` for full details.”). This directs the user to where they can read all the findings.
    - **Key Findings:** Briefly note the categories of the most significant issues in the summary (for example, “including inefficient query patterns and heavy memory usage in one module”) to give the user an idea of what areas need attention. Keep this high-level; the full explanations are in the report.
    - **Next Steps:** Provide constructive guidance on what to do with the results. Encourage the developer to **address the most critical bottlenecks** first. For instance, if a severe N+1 query issue was found, they should consider refactoring that database access logic promptly. If many minor issues were noted, they might prioritize which optimizations to implement based on impact. Suggest that after making improvements, the developer can run `/performance-audit` again to verify that the issues have been resolved or new ones haven’t been introduced.
    - If the project has related commands or tools, you can mention them. For example, if a `/fix-issue` or `/optimize` command exists for automated fixes, recommend using it on specific findings (otherwise, the developer will handle changes manually). Emphasize that these suggestions are based on static analysis and heuristics – they should be validated with real testing or monitoring. Encourage the team to perform runtime profiling (outside of Claude, since this command doesn’t run code) on critical paths if needed, to confirm the impact of these potential bottlenecks.
    - Throughout the summary, maintain a professional and supportive tone. The goal is to inform the developer of performance concerns without alarmism, and to empower them with clear suggestions on how to make the application more efficient.

**Example:** If a developer runs `/performance-audit`, the assistant will scan the entire repository’s code for performance issues. It might find, for instance, a couple of inefficient nested loops in the data processing module, an N+1 database query in the user service, and a large image file loaded uncompressed on the front-end. The command will create a report file such as `.claude-sdlc/performance/2025-07-12T16-05-30-performance-audit.md` outlining each issue (grouped under **Loops**, **Database**, **Frontend**, etc.), why it could hurt performance, and suggestions to improve it. The chat output might be: **“✅ Performance audit complete. Found 5 potential issues (including 2 heavy loops and 1 inefficient DB query). See `.claude-sdlc/performance/2025-07-12T16-05-30-performance-audit.md` for details.** Consider addressing these bottlenecks; for example, optimize the nested loops and batch the database queries. After improvements, you can run `/performance-audit` again to ensure all major performance concerns are resolved.”