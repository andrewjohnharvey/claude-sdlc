# Architecture Review

Analyzes the project’s overall system design by reviewing architecture documentation and the codebase structure to assess design quality, scalability, and alignment with specifications.

## Instructions

1. **Initialize Review Context**
    - **Scope Determination:** If the `/architecture-review` command is run with an argument, interpret it as a focus area (for example, the name of a subsystem or a specific architecture document to review). Narrow the analysis to that context – open the relevant design doc and concentrate on that portion of the codebase. If no argument is provided, prepare to review the entire project’s architecture.
    - **Gather Documentation:** List the existing design docs in the `.claude-sdlc/architecture/` directory to see what information is available. Use **!**`ls .claude-sdlc/architecture` to show all architecture files. Open any core architecture documents (or the one matching the focus area) to understand the intended system design, key components, and architectural decisions recorded. These files might include system overviews, module descriptions, UML diagrams, etc.
    - **Correlate Docs with Code:** If focusing on a particular component or module, search for its references in both documentation and code to link them. For example, run **!**`grep -i "Auth" .claude-sdlc/architecture/*` to find mentions of an "Auth" component in the docs, and use **!**`grep -R "Auth" .` in the codebase to locate where authentication is implemented. This helps identify how the documented design maps to the actual code structure.
    - **Codebase Overview:** Get a high-level overview of the code organization to infer the implemented architecture. For instance, list top-level directories using **!**`find . -maxdepth 1 -type d` (or a similar command) to reveal major folders (e.g., `frontend/`, `backend/`, `services/`, `database/`, etc.). You might also run **!**`find . -maxdepth 2 -type d` to see the second-level structure. Note how the project is partitioned – this will indicate whether the architecture is monolithic, multi-tiered, microservice-based, plugin-oriented, etc.
    - **Check for Documentation Gaps:** As you gather context, note if any architecture documentation is missing or clearly outdated. If the `.claude-sdlc/architecture/` directory is empty or sparse, or if the docs do not reflect parts of the system you see in code, plan to flag this as an issue. (For example, if you find a `payments/` module in code but no mention of it in any design doc, that indicates documentation is not up-to-date.) Proceed with the review using the code as the source of truth in such cases, but ensure to record the lack of documentation in the findings.
2. **Perform Architecture Analysis**
    - **Modularity & Separation of Concerns:** Examine how well the system is divided into modules or layers and whether each component has a clear responsibility. Identify any areas where concerns are not well separated – for instance, modules that handle many unrelated tasks or business logic mixed with UI or data access code. Note if there are clear boundaries between different layers (presentation, application logic, data/storage) or services, and call out any tangling of responsibilities or tight coupling that might hinder maintainability.
    - **Complexity:** Assess the complexity of the architecture. Look for areas that seem overly complicated or convoluted. This could be an excessive number of interdependent services, deeply nested sub-systems, or reliance on complex configurations to make components work together. Identify if there are circular dependencies, spaghetti connections between modules, or a needlessly elaborate design where a simpler approach would suffice. High complexity can indicate the architecture might be hard to understand, extend, or debug, so highlight places where refactoring or simplification could reduce complexity without losing needed functionality.
    - **Scalability:** Evaluate whether the design can scale with increased load, data volume, or number of users. Pinpoint any **bottlenecks** or **single points of failure** in the architecture. For example, a central database or service that all components must sync through could limit horizontal scaling, or a module that runs on a single thread could become a throughput bottleneck. Check if the system is distributed and load-balanced where appropriate, or if it relies on a monolithic component that might struggle under growth. Also consider if the architecture has provisions for scaling (caching layers, asynchronous job queues, replication, etc.). Any element that would prevent the system from scaling smoothly (or would cause the whole system to fail if it goes down) should be noted, along with suggestions to mitigate these risks (like adding redundancy or decoupling components).
    - **Conventions & Consistency:** Review how consistently the architecture adheres to intended design patterns and project conventions. If the project claims a specific architectural style (e.g., MVC, microservices, event-driven), check that it’s implemented uniformly. For example, ensure that all services follow a similar template or that all modules use consistent naming and interactions. Look for any outlier implementations that don’t fit the pattern (like one service that bypasses the standard API gateway or one module that doesn’t follow the common naming scheme). Consistency in architecture makes the system easier to understand and reduces integration issues, so flag any inconsistencies or deviations from the established conventions. Also, check for adherence to any industry best practices relevant to the project’s tech stack (e.g., using layered architecture, repository patterns, dependency injection, etc., if those are standard for this kind of project).
    - **Design Alignment:** Compare the actual implemented architecture (what you see in the code structure and module interactions) with what’s described in the design documentation or specifications. Identify any mismatches or gaps. For instance, if the documentation outlines 5 microservices but the codebase has 7 services (or vice versa), note that discrepancy. Or if the docs specify a particular technology or pattern (say, an event bus for communication) and the code doesn’t use it (perhaps using direct HTTP calls instead), highlight that. Conversely, if the code has architectural elements not mentioned in the docs (like a new caching layer added in code without an accompanying design update), that implies the docs are outdated. This analysis will reveal whether the team’s implementation aligns with the planned architecture or if the system has drifted away from the documented design. Also, confirm whether all major architectural concerns (security, fault tolerance, data integrity, etc.) are addressed either in design or implementation; if something important is undocumented and unclear (for example, how failover is handled), bring that up as well.
3. **Compile Architecture Review Report**
    - **Create Report File:** Compile all your findings into a structured Markdown report. Create a new file in the project under `.claude-sdlc/reviews/` with a filename that includes the current date/time and the phrase `architecture-review`. For example: `.claude-sdlc/reviews/2025-07-12T160000-architecture-review.md`. (Use a timestamp format that is sortable and unique, such as ISO 8601 date-time.) This ensures each architecture review is saved separately and chronologically. If the `reviews/` directory does not exist yet, create it before saving the file.
    - **Organize by Theme:** In the report file, organize the findings into sections by the themes evaluated. Use clear section headings (e.g., **Modularity**, **Complexity**, **Scalability**, **Conventions**, **Design Alignment**) to structure the report. Under each heading, list the specific observations and issues identified for that category:
        - For each issue or observation, provide a brief but clear description. Whenever possible, reference relevant parts of the code or documentation to give context – for example: “**Modularity:** The `UserManagement` module is handling both user authentication and email notifications, which mixes concerns that could be split into separate services.” Or: “**Scalability:** All incoming jobs are handled by a single `JobProcessor` instance running on one server, creating a single point of failure and a potential throughput bottleneck.”
        - Alongside describing the issue, offer a recommendation or best practice to address it. Keep the tone factual and solution-oriented. For instance: “Introduce a dedicated email service or queue to offload notification handling from the `UserManagement` module, improving modularity,” or “Consider implementing a distributed job queue with multiple workers to eliminate the single-thread bottleneck in `JobProcessor` and improve scalability.”
        - If there are areas where the architecture excels or follows best practices, note those as well (to provide a balanced view). For example, under **Conventions**, you might note that “All services consistently use the same request/response format, which is good for maintainability,” or under **Modularity**: “The separation between the web frontend and core API is clean, with well-defined interfaces.”
    - **Documentation Findings:** Make sure to include any issues related to the architecture documentation itself, likely under the **Design Alignment** section (or in an introductory summary). For example, if you discovered missing or outdated docs, clearly state that: “The architecture documentation has not been updated to include the new payment service module present in the code, indicating a documentation gap.” Emphasize the importance of updated docs as part of architectural integrity.
    - **High-Level Summary:** Begin the report with a short summary that gives an overview of the architecture’s state. For instance: “Reviewed the project’s architecture: found 2 major scalability risks and a few minor modularity issues; overall the system design is well-structured, following a microservices approach with clear API boundaries, but some documentation is outdated.” This summary should enumerate the key findings at a glance (including positives and negatives). It helps stakeholders quickly grasp the health of the architecture.
    - **Save the Report:** After writing down all the categorized findings and recommendations, save the Markdown file to the repository. **Do not make any direct changes to the project’s source code or config as part of this review.** The purpose of the `/architecture-review` command is to analyze and report, not to implement fixes. All changes or improvements should be handled separately (e.g., via feature plans and subsequent builds). The saved report will serve as a reference for the team to address the identified issues.
4. **Optional: Plan Architecture Improvements**
    - **Identify Needed Changes:** If the architecture review uncovers significant issues that warrant code or design changes (for example, a critical scalability limitation, a poorly modularized component that impacts many features, or missing fault-tolerance that poses a risk), consider initiating a follow-up action plan. Often, non-trivial architecture fixes should be managed as their own feature or refactoring effort. Determine which findings rise to this level – typically those that would require multiple steps or significant development work to resolve.
    - **Leverage Feature Planning:** For each major recommended improvement, you can either **suggest** that the developer create a feature plan (using the `/create-feature` command) or automatically scaffold one if appropriate. For instance, if the review highlights that the authentication system needs refactoring to separate concerns, you might suggest: “Consider running `/create-feature "Refactor authentication module for better separation of concerns"` to plan out this improvement.” Alternatively, if you have enough confidence and clarity on what needs to be done, you can generate the feature plan file directly:
        - **Generate Feature Plan (if appropriate):** Choose a concise identifier for the improvement (e.g., `refactor-auth-module` or `introduce-caching-layer`). Create a new Markdown file in `.claude-sdlc/features/` named `<identifier>.md` (for example, `.claude-sdlc/features/refactor-auth-module.md`). In that file, outline a checklist of actionable tasks to implement the architectural improvement, following the standard format (`- [ ] Task description`). Each task should be atomic and specific. For example:  
            `- [ ] Extract email notification logic from UserManagement module into a new service`  
            `- [ ] Implement a Redis cache to store session data and update auth module to use it`  
            `- [ ] Update architecture docs to include the new service and caching layer`  
            This turns the high-level recommendation into a concrete plan that can be executed with `/build`.
        - If you create such a feature plan automatically, ensure it is clearly tied to the findings of the review (perhaps mention in the plan or in the report which issue it addresses). This way, the team can easily trace why the new feature/refactor was proposed.
    - **Use Judgment for Automation:** Only auto-generate a feature plan if the needed improvement is well-defined and you’re confident it should be done. In many cases, it’s better to let the human decide on next steps (the report itself may be sufficient for them to act on). The assistant should use this capability sparingly and preferably with the user’s implicit approval (for example, if the user ran an `architecture-review` expecting guidance, they might welcome an auto-created plan for an obvious fix). When in doubt, simply include the recommendation in the report and advise the user to use `/create-feature` for it. The goal is to ensure important architectural issues are not only reported but also captured as actionable tasks for the development team, without overstepping by making large changes unilaterally.
5. **Summarize and Guide**
    - **Communicate Results:** After saving the architecture review report (and any generated feature plans, if applicable), provide a summary of the findings in the chat to guide the developer. Start by confirming that the architecture review is complete and give a brief overview of the outcomes. For example: _“✅ Architecture review complete. The system is generally well-structured, with clear service boundaries, but we identified a couple of scalability concerns and some inconsistencies with the design docs.”_ This lets the user know at a glance what to expect.
    - **Reference the Report:** Mention that a detailed report has been saved in the project, and provide the path to it. For instance: _“See **.claude-sdlc/reviews/2025-07-12T160000-architecture-review.md** for the full analysis organized by Modularity, Complexity, Scalability, Conventions, and Design Alignment.”_ Ensure the path is accurate so the user can quickly open the file. Highlight that the report contains specific issues and recommendations under each category.
    - **Next Step Recommendations:** Based on the findings, guide the developer on what to do next. For serious issues or omissions, encourage prompt action. For example, if a single point of failure was found, you might say: _“It’s recommended to address the single point of failure in the job scheduler; you can plan this out via `/create-feature` or handle it in an upcoming sprint.”_ If documentation was outdated, advise updating it: _“The architecture docs need to be updated to reflect the current system (noted in the report); keeping documentation current is important for future reviews and onboarding.”_ If you went ahead and created a feature plan file for an improvement, point that out: _“I’ve created a draft feature plan at **.claude-sdlc/features/refactor-auth-module.md** to refactor the auth system as discussed. You can review and refine it, then run `/build refactor-auth-module` when ready to implement.”_
    - **Encourage Follow-Up:** Emphasize the benefits of addressing the architectural issues. The tone should be constructive: the idea is to improve the system’s robustness and maintainability. If appropriate, suggest running related commands afterwards (for example, after implementing changes, they might run `/code-review` on the affected modules, or run `/architecture-review` again in the future to verify all recommendations were implemented correctly). Remind the developer that maintaining a solid architecture is an ongoing process, and this review is meant to proactively catch design problems early. By following the recommendations and keeping architecture documentation up-to-date, the team can avoid bigger issues down the line and ensure the project remains scalable and easy to evolve.
**Example:** If a developer runs `/architecture-review`, the assistant will review both the documentation and the code structure of the project. For instance, it might open a file like `.claude-sdlc/architecture/system-design.md` and discover that the document describes a separate microservice for payments, even though the codebase doesn’t actually have a dedicated payment service (perhaps all payment logic ended up in the monolithic API server). It also examines the code and notices that a `TaskScheduler` class is responsible for all background jobs without any backup process — a potential single point of failure. After analyzing these and other factors (e.g., noticing that most modules follow an MVC pattern except one that doesn’t, or that the overall design is consistent with documented APIs), the command generates a Markdown report (for example, `.claude-sdlc/reviews/2025-07-12T160000-architecture-review.md`). In that report, under **Design Alignment**, it flags the discrepancy regarding the missing Payment service (suggesting the docs or implementation are out of sync). Under **Scalability**, it highlights the risk of the lone `TaskScheduler` and recommends introducing a job queue with multiple workers. Under **Modularity**, it might praise that the project is divided into clear services for users, orders, and inventory, noting good separation of concerns there. Once the report is saved, the assistant posts a summary in the chat, perhaps saying: _“✅ Architecture review complete. Found a few issues: an outdated design doc (Payment service not implemented as described) and a scalability risk in the job handling. Overall architecture is solid and adheres to its microservice design in most areas. See `.claude-sdlc/reviews/2025-07-12T160000-architecture-review.md` for details.”_ It might then advise the developer to update the documentation and consider creating a feature to add a dedicated payment service or improve the job scheduler (for example, suggesting to run `/create-feature "Add payment microservice"` or similar). The developer can then use that guidance to improve the project’s architecture, ensuring it stays aligned with best practices and the project’s needs.