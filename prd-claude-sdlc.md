# Claude-SDLC PRD

## Overview

Claude-SDLC is an AI-assisted Software Development Life Cycle (SDLC) command suite for Anthropic’s Claude Code environment. It provides a set of custom slash commands that streamline common development workflows by breaking them into consistent, well-defined steps driven by AI. Instead of ad-hoc prompts, Product Managers get structured, repeatable processes for tasks like code review, feature implementation, architecture analysis, and more Claude Command Suite. The suite includes commands for comprehensive code reviews, end-to-end feature development, architecture design reviews, security audits, structured issue resolution, and performance optimization reddit.comreddit.com. Each command leverages Claude’s large context window and tool use capabilities to read project files, run tests or git commands, and generate code or reports as needed, all within the developer’s project workspace. This document outlines the product requirements for Claude-SDLC, covering its goals, architecture, installation, command behaviors, security considerations, and future enhancements in a format accessible to developers and product managers.

## Goals

- **Automate Repetitive SDLC Tasks:** Reduce manual effort in planning, coding, reviewing, and testing by automating these steps with AI-driven commands. Developers should be able to invoke a single command to perform tasks that would normally require multiple steps (e.g., performing a full code review or scaffolding a new feature).
- **Ensure Consistent Best Practices:** Encapsulate software engineering best practices into the command workflows. Each command follows a systematic approach based on industry and Anthropic best practices, breaking complex tasks into manageable steps to ensure thoroughness and consistency across projectsreddit.com.
- **Improve Development Speed and Quality:** By handling boilerplate and analysis tasks, Claude-SDLC commands let developers focus on high-level decisions. The AI can quickly fix linting/type errors, run tests, or analyze design patterns, leading to faster iterations without sacrificing quality.
- **Leverage Project Knowledge:** Keep development context centralized. The commands will use project-specific data (existing architecture docs, coding standards, test results, etc.) from the repository’s `.claude-sdlc` folder to make informed decisions. This ensures the AI’s output aligns with the project’s design and requirements.
- **Seamless Integration with Claude Code:** Provide a frictionless user experience within Claude Code’s CLI/IDE environment. Installation and usage of the commands should be simple, with no complex setup. Developers invoke commands as `/create-feature`, `/build`, `/code-review`, etc., and the system handles the rest.
- **Internal Exploratory Rollout:** This initial release is rolled out for internal use only (an exploratory pilot). Success will be measured through internal adoption and feedback rather than public KPIs or user counts, focusing on learning and refinement within the team.

## System Architecture
Claude-SDLC builds on Claude Code’s custom _project slash command_ functionality docs.anthropic.comdocs.anthropic.com. Each command is defined as a Markdown file in the project’s repository under a special directory. Claude Code automatically recognizes any Markdown file in the project’s `.claude/commands/` directory as a slash command (with the filename as the command name) github.com. In this suite, all command files reside at the top level of `.claude/commands/` (no subdirectories or namespace prefixes), which means they are invoked with simple names like `/create-feature` rather than a namespaced format. This design choice ensures the commands are scoped to the current project directory and avoids naming conflicts or user-wide commands, since each project includes its own copy of these commands.

A separate folder, `.claude-sdlc/`, is used to maintain project-specific SDLC data and outputs. Within this folder, there are subdirectories for different types of information:
- **Features (`.claude-sdlc/features/`):** Each planned feature has a Markdown file (e.g., `user-auth.md`) containing the _atomic task list_ (checklist) required to implement that feature. This serves as the to-do list that the AI will execute.
- **Architecture (`.claude-sdlc/architecture/`):** This holds architecture and design documentation (design specs, API contracts, ERDs, etc.). Commands like feature creation or build will read these files to understand the broader context (for example, knowing existing design patterns or data models)reddit.com. Leveraging these docs allows the AI to align its implementations with the intended architecture.
- **Builds (`.claude-sdlc/builds/`):** This directory stores build reports and logs for each execution of the build process (e.g., `user-auth-2025-07-12T11-38-00.md`). These reports include summaries of tasks completed, changes made, test results, and any errors encountered during a `/build` run, providing traceability for each feature implementation.

**Command Execution:** When a user triggers a Claude-SDLC slash command, Claude Code loads the corresponding Markdown prompt which contains the structured instructions for that workflow. The command can reference the project files and even run shell commands as needed. For example, commands may use Claude Code’s ability to execute CLI commands (via `!` in the prompt) and include their output in context docs.anthropic.com docs.anthropic.com. This allows the AI to compile code, run tests, perform git operations, etc., as part of its autonomous loop. (E.g., the `/build` command might run `npm run build` or `pytest` to verify each task’s outcome, as allowed by the command’s tool permissions). Claude Code is adept at running CLI commands and reading their output, making such feedback loops straightforward to implement pulsemcp.com.

**Sub-agent Parallelism:** The architecture supports executing multiple tasks in parallel using Claude sub-agents. Claude Code can spawn parallel agent processes (sub-agents) to work on independent tasks simultaneously, especially useful for large features with independent components. For instance, if a feature’s tasks include updating backend code and frontend code that don’t depend on each other, the `/build` command could dispatch these to separate Claude sub-agents in parallel. This is achieved by the main command prompt orchestrating child prompts (via background threads or asynchronous calls in the environment) for each task. Each sub-agent works within the project context and reports back when done. This approach accelerates the build process by leveraging Claude’s ability to handle concurrent threads of execution.

**State and Context Management:** All commands are designed to be _idempotent_ and stateless beyond the files they read/write. The source of truth for feature progress is the feature’s Markdown checklist in `.claude-sdlc/features/`. As tasks complete, the checklist is updated, which persists the state. If a command is re-run, it can read the checklist to know which tasks remain. Commands also consult existing architecture docs and prior build reports to avoid duplicate work and maintain consistency with past decisions. By keeping context in files (architecture docs, checklists, reports), the AI’s reasoning is enriched and persists across sessions. For example, when implementing a feature, the `/build` command will check for any relevant `.claude-sdlc/architecture/*.md` documentation to inform its implementation decisions (such as following an established design pattern or interface) before writing new code.

_Note:_ Claude Code’s default infrastructure covers concurrency control and environment isolation. The suite does not set any custom concurrency limits; it leverages Claude’s default allowances for parallel sub-agents. Similarly, commands run within Claude’s sandboxed project environment, so no additional isolation measures are needed.

## Installation

Setting up Claude-SDLC in a project is straightforward:
1. **Prerequisites:** Ensure you have Anthropic’s Claude Code environment set up (Claude Code v1.0+ github.com, accessible via VS Code extension or the CLI). The project repository should be initialized as a Claude Code project (usually containing a `CLAUDE.md` memory file and connected to Claude Code). The underlying Claude MCP server connectivity is already in place by default, requiring no extra setup for Claude-SDLC.
2. **Obtain Command Files:** Acquire the Claude-SDLC command suite files. These are Markdown files for each command (e.g., `create-feature.md`, `build.md`, `code-review.md`, etc.). They can be obtained by cloning the Claude-SDLC repository or via an installer script. _(For example, an interactive installer could clone a GitHub repo and copy the files automatically.)_
3. **Project Installation:** Place the command files into the project’s `.claude/commands/` directory. If this directory doesn’t exist, create it at the root of your repository. Copy all the Claude-SDLC `.md` files there. Because these are **project-specific commands**, they will be loaded only for this project (when you run `/help` in Claude, they’ll appear with "(project)" in the description docs.anthropic.comdocs.anthropic.com). No global installation is needed or desired – the commands run in the context of the current project only.
4. **Initialize SDLC Folders:** Create the `.claude-sdlc/` directory structure in your repository. At minimum, create subfolders for `features/`, `architecture/`, and `builds/` inside `.claude-sdlc/`. These can be empty initially. Optionally, you might add initial architecture docs or planning docs here if available.
5. **Configuration:** The commands are designed to work out-of-the-box. However, you can customize them if needed. Each command file is Markdown with a clear structure (title, description, steps) github.com. For example, you could edit the allowed tools in YAML frontmatter or tweak instructions to better fit your tech stack (the commands use Claude Code’s standard frontmatter schema for command metadata). By default, the provided commands are generic and should work with any language or framework github.com.
6. **Verification:** Launch Claude Code in the project (e.g., open the project in VS Code with Claude Code enabled, or use the CLI). Run `/help` to verify the new commands are listed. You should see commands like `/create-feature`, `/build`, etc., available (marked as project commands). You can now invoke them to perform their respective tasks.

_Note:_ All command names are flat (no prefix or grouping) to avoid confusion and ensure simplicity. In earlier iterations, commands were namespaced with a `project:` prefix (e.g. `/project:create-feature`), but this has been removed in favor of per-project command installation. Now the command you invoke clearly applies to the current project context by virtue of being in the project’s `.claude/commands` folder. Multiple projects can use the Claude-SDLC suite without conflict, as each maintains its own command files and `.claude-sdlc` data.

## Slash Commands
This section details each major slash command in the Claude-SDLC suite, describing its purpose, behavior, and usage. All commands assume they are run at appropriate times (for example, running `/build` after a feature’s tasks have been planned, or running `/code-review` when you have code changes ready or want a full codebase audit). Commands may prompt the user for confirmation or clarification in ambiguous cases, but generally strive to operate autonomously using the information available in the repository.

### `/create-feature` – Feature Planning & Definition

The `/create-feature` command initiates a full **feature development workflow** up to the planning stage. It takes as input a short feature name or description (provided as the command argument) to identify the new feature. For example: `/create-feature user-authentication` would begin planning for a “user authentication” feature.

**Behavior:**
1. **Requirement Analysis:** The command reads any provided details (the argument or additional prompt context) about the feature. It may ask follow-up questions if the feature description is too vague, to ensure it has a clear understanding of scope and requirements.
2. **Design Outline:** Claude analyzes how this new feature fits into the existing system. It looks for relevant design or architecture information in `.claude-sdlc/architecture/`. For instance, if there is an architecture doc for the module or similar features, it will incorporate that context. It then outlines the high-level approach for the feature – e.g., which components need changes, any new modules or APIs, changes to data models, etc. If needed, it may create or update an architecture document (e.g., `.claude-sdlc/architecture/user-authentication-design.md`) to record important design decisions or diagrams (in Markdown form) for the feature.
3. **Task Breakdown:** The AI decomposes the feature into a list of atomic development tasks. Each task should be a small, actionable step like “Create database migration for users table”, “Implement login API endpoint”, “Integrate frontend login form with API”, “Write unit tests for authentication service”, etc. The tasks are ordered logically and kept concise (no extraneous metadata). Claude ensures the tasks cover implementation, testing, and documentation updates as needed, achieving an end-to-end development plan reddit.com. Dependencies or parallel execution cues are noted only when they add clarity (rather than tagging every task by default).
4. **Feature File Creation:** The command creates a new Markdown file under `.claude-sdlc/features/` named after the feature (e.g., `user-authentication.md`). In this file, it writes the task list as a checklist. Each task is prefixed with an unchecked box (`- [ ] Task description`). For example:
    - `[ ] Create User model and database migration for user accounts`
    - `[ ] Implement authentication API (login & logout)`
    - `[ ] Send verification email after registration`
    - `[ ] Write unit tests for login and registration flows`  
        and so on.
5. **Initial Scaffolding:** If appropriate, `/create-feature` can also scaffold some initial boilerplate to set the stage for development. For instance, it might create empty files or stub functions where the feature code will go (to give the builder a starting point), or update a configuration file to include placeholders. However, it does not fully implement the feature – actual coding is left to the `/build` step. The scaffolding is minimal and only done if it aids clarity (the primary output is the plan, not code).
6. **User Guidance:** Once the plan is ready, Claude outputs a summary of the feature plan to the user. It might render the task checklist and brief design notes in the chat, explaining the overall plan. It will then suggest the next step: typically, to proceed with implementation by running the `/build <feature>` command for the newly created feature plan. This makes it clear how to move from planning to execution.

Overall, `/create-feature` gives teams a structured development plan in minutes. It ensures no major aspect is forgotten (code, tests, docs, security, etc., as applicable to the feature). The plan can be reviewed and adjusted by the human developer before coding begins. By capturing the plan in a file, it also serves as documentation for the feature’s development and can be referred to in code reviews or future maintenance.

### `/build` – Automated Task Execution (Feature Implementation)
The `/build` command is a new addition to Claude-SDLC, responsible for **automating the implementation of a planned feature**. Given a feature name, it will locate the corresponding task checklist in `.claude-sdlc/features/` and carry out each task sequentially (and in some cases, in parallel) by generating code, running commands, and updating documentation. For example, after planning “user-authentication” via `/create-feature`, running `/build user-authentication` will begin executing the tasks in `user-authentication.md`. It handles ancillary aspects like database migrations, background service setup, and secret management through AI defaults, requiring no special configuration from the developer.

**Scope & Behavior:** The `/build` process is designed to be as detailed and thorough as a human engineer implementing the feature, with the ability to self-correct errors. It performs the following steps:
1. **Load Feature Tasks:** Read the atomic task list from the feature’s Markdown file (e.g., `.claude-sdlc/features/user-authentication.md`). This provides the ordered to-do list for implementation. Each task is treated as a sub-goal for the AI.
2. **Gather Context:** Before executing tasks, `/build` loads relevant context:
    - It opens any pertinent architecture/design docs from `.claude-sdlc/architecture/` (for example, the `user-authentication-design.md` if it exists, or other global architecture guidelines) to inform its implementation approach. This helps the AI follow the intended design and understand system constraints.
    - It may also check for existing code references or similar features in the codebase to maintain consistency (e.g., how a “user” model is defined elsewhere).
    - If it doesn't find the needed architecture guidelines it will ask if you want to build one for this feature
3. **Task Execution Loop:** The command iterates through each task in the checklist and attempts to complete it:
    - For each task, Claude may spawn a **sub-agent** to handle the task. Sub-agents are essentially parallel Claude Code instances that can operate concurrently. If tasks are independent, `/build` can run a few sub-agents in parallel to speed up development (for example, working on frontend and backend changes simultaneously). Otherwise, it executes tasks one by one in sequence.
    - The sub-agent responsible for a task will write the necessary code or perform the required actions for that task. Claude has access to the workspace, so it will open/create/edit code files as needed to implement the feature. It will also utilize tools – e.g., running compilation or tests – to verify the task’s success. (Claude Code can compile and run tests as part of its workflow pulsemcp.com, so the agent uses those capabilities to check its work).
    - After completing a task’s implementation, the AI marks the task as done by updating the checkbox in the feature file to `[x]`. This is written back to `.claude-sdlc/features/<feature>.md`, so the checklist on disk reflects progress in real time.
4. **Clarification Questions:** If at any point Claude wants to clarify anything, it can ask the user for additional information.
5. **Error Handling & Pausing:** If a task fails or encounters an error, the `/build` command will **pause and alert the user**:
    - For example, if compiling the code after Task 3 results in a compiler error or a unit test fails, the sub-agent reports the error (with relevant logs or messages) to the main agent. The main agent then stops further tasks.
    - The incomplete task remains unchecked, and Claude presents the error details to the user in the chat. At this point, it may ask for guidance or confirmation on how to proceed. The user can choose to fix the issue manually, instruct Claude on how to resolve it, or allow Claude to attempt a fix.
    - This pause mechanism ensures that the build process does not blindly continue in a broken state. It prioritizes not introducing faulty code. The user’s intervention acts as a safeguard and an opportunity to adjust plans if needed.
6. **Resuming After Fixes & Clarification Questions:** If the user provides instructions or if Claude is confident in an automated fix, the `/build` process will resume. It will re-attempt the failed task (or adjust the plan) and once successful, proceed to subsequent tasks. This cycle can repeat for multiple failures – the aim is to systematically address issues until all tasks are completed or the user aborts.
7. **Parallel Task Coordination:** In cases where multiple sub-agents are running (parallel tasks), `/build` monitors all threads. If one sub-task fails, others might either be paused or continue if unaffected, depending on the nature of the tasks. The system is careful to avoid conflicts (e.g., two agents editing the same file). Typically, tasks chosen for parallel execution will be ones operating in different parts of the codebase. The build report (next step) will note which tasks were done in parallel and their outcomes.
8. **Build Report Generation:** Upon completion (when all tasks are successfully done and checked off), `/build` generates a comprehensive **build report**. This is saved as a Markdown file under `.claude-sdlc/builds/`, named with the feature and timestamp (e.g., `user-authentication-2025-07-12T11-45-00.md`). The report includes:
    - A copy of the final task checklist (with all items checked).
    - A summary of actions taken for each task (e.g., “Created file X, modified Y, ran tests Z”), including code snippets of important changes or results of running tools.
    - Any errors that were encountered and how they were resolved.
    - The outcome of the feature build, such as “Build successful, all tests passed, ready for review.”
    - Optionally, it may include diff stats or a link to a commit if the command also committed the changes.
    - It will also update information in other folders. For example it will update any relevant documents in the `/architecture` folder.
9. **Completion Message:** Claude also outputs a summary in the chat upon finishing `/build`. It will inform the user that the feature implementation is complete, point to the build report file for details, and recommend next steps (for example, running `/code-review` to do a final review of the changes, then instructing the user to run the test suite or open a pull request).

The `/build` command essentially acts as an AI-driven developer that reads the plan and writes the code. It significantly accelerates the development cycle by automating coding and verification. Importantly, it stops when something goes wrong, ensuring that errors are caught early. With the detailed build report, a human developer can easily verify what was done or troubleshoot any remaining issues. The design of `/build` emphasizes reliability and transparency so that using it feels like working with a diligent junior developer who documents everything and asks when uncertain.

### `/code-review` – Comprehensive Code Review

The `/code-review` command performs an in-depth analysis of the codebase or a set of changes, acting as an AI pair reviewer. Its goal is to assess code quality, correctness, security, and performance, and then provide actionable feedback and recommendations.

**Behavior:**
- When invoked with no arguments (`/code-review`), it will analyze the entire repository (or the main portions of the codebase) for any issues or improvements. If invoked with a specific target (for example, a path or a recent diff), it will focus on that scope – e.g., `/code-review backend/` could focus on the backend directory, or `/code-review PR#15` (in context) could review the changes in a pull request.
- Claude gathers context by reading the code files. It may not read _every single file_ if the project is huge; often it will prioritize recently changed files or core modules. It can also run static analysis tools if configured (though by default it uses its own analysis capabilities). The command ensures it’s operating on up-to-date code by possibly running `git diff` or `git status` to see uncommitted changes, staging differences, etc., including those just produced by a `/build`.
- The code review output is structured and thorough. Typically, it includes:
    - **Summary:** A high-level summary of the codebase’s health or the changes reviewed (e.g., “Overall, the code conforms to standards and is well-structured, with a few potential issues noted below.”).
    - **Style & Maintainability:** Notes on code style inconsistencies, dead code, complex functions that might need refactoring, or areas lacking documentation. The AI ensures consistency with any style guidelines observed in the project.
    - **Correctness & Bugs:** Identification of possible logical errors or bugs. For example, code paths that might throw exceptions, off-by-one errors, incorrect algorithmic approaches, etc. Claude uses its understanding of the code and problem domain to infer where behavior might not meet intentions.
    - **Security Assessment:** A mini security audit is included as part of code review reddit.com. The command flags common security pitfalls (e.g., SQL injection risks, use of outdated cryptography, improper error handling that leaks info, etc.). It also checks dependency versions against known vulnerabilities if it has access to such information (this might require an up-to-date dependency list and known CVEs, which the AI can attempt to recall or the system can provide).
    - **Performance Considerations:** It points out any obvious performance issues (like nested loops that could be expensive, suboptimal database queries, large in-memory operations, etc.). If any part of code could become a bottleneck, the review will highlight it and possibly suggest optimizations or at least note it for further testing reddit.com.
    - **Testing & Coverage:** The review looks at the state of tests. If the project has tests, it notes areas of code that changed without corresponding tests, or critical functionality that lacks tests. It may suggest additional test cases for robustness. If the project has no tests, it will recommend introducing some (this might tie in with a future enhancement or a separate command to generate tests).
    - **Specific Change Feedback:** If reviewing a specific diff or recent changes (say after a build), it will comment on those changes specifically – e.g., “The new authentication feature is well-integrated; consider adding input validation on the email field,” or “Function X was modified; however, its error handling might need improvement to cover Y case.”
- The output of `/code-review` is provided in the chat, and it is organized by categories for clarity (security issues, bugs, suggestions, etc.). This makes it easy for a developer to scan and identify critical vs. minor points.
- The command does _not_ automatically change code (to keep the review process separate from implementation). However, it may offer code snippets as suggestions for how to fix an issue. For instance, “Consider using parameterized queries here to prevent SQL injection. For example:” and then show a snippet.
- The review results are also saved to a Markdown file (e.g., `.claude-sdlc/reviews/<date>-review.md`) for record-keeping by default. This ensures an archived report is available for later reference alongside the interactive feedback.
- The thoroughness or strictness of the review can be adjusted based on the user’s instructions. For example, a developer can request an extra strict review (catching even minor issues) or a more lenient overview, and Claude will adapt accordingly in its analysis.
- Using `/code-review` before merging code ensures that any AI-generated or human-written changes are vetted. It acts as a sanity check that encourages higher code quality and knowledge sharing (as the AI often explains _why_ something is an issue, educating team members).

In essence, `/code-review` serves as an AI code auditor covering quality, security, and performance in one go reddit.com. This helps catch issues early and gives developers greater confidence in the codebase’s integrity.

### `/security-audit` – Security Audit and Hardening

The `/security-audit` command focuses on analyzing the codebase (and project configuration) for security vulnerabilities and providing guidance on fixing them. While `/code-review` includes a security pass, `/security-audit` delves deeper specifically into security concerns.

**Behavior:**
- Claude scans through code, configuration files, and dependencies specifically hunting for security issues. This includes:
    - Insecure coding patterns (use of obsolete cryptographic functions, hard-coded secrets, lack of input sanitization, etc.).
    - Authentication and authorization logic issues (e.g., missing access control checks).
    - Error handling that might leak sensitive information (stack traces, etc.).
    - Dependency checks: It reviews dependency files (like `package.json`, `requirements.txt`, etc.) for known vulnerable library versions and suggests upgrades if needed (based on its training knowledge of CVEs up to 2025).
    - Infrastructure as code or deployment configs for security (if present, e.g., checking Dockerfile or AWS config for best practices).
- The command might utilize any existing threat model or security guidelines in the `.claude-sdlc/architecture/` docs. For example, if there’s a security design doc, it will ensure the code meets the specified security requirements or update that doc if discrepancies are found.
- The output is a detailed report of findings:
    - **Vulnerabilities Found:** Each issue is listed (e.g., “SQL Injection possible in `UserController.cs` line 45 due to string concatenation in SQL query.”).
    - **Severity & Impact:** Claude assigns a severity level (Critical/High/Medium/Low) to prioritize.
    - **Recommended Fix:** Specific guidance is given for each issue (continuing the example: “Use parameterized queries or an ORM to handle user inputs safely.”).
    - **General Hardening Suggestions:** Beyond specific bugs, it might recommend general improvements such as “Implement rate limiting on login API to mitigate brute force attacks” or “Use HTTPS in local development to mirror production security” etc., if relevant.
- The audit might be time-consuming (as it’s thorough), so it could optionally save the report to `.claude-sdlc/builds/security-audit-<date>.md` or similar, though primarily it outputs to chat.
- After addressing issues, developers can re-run `/security-audit` to verify all high-risk issues have been resolved. It’s especially useful before a release or after major changes to ensure new vulnerabilities haven’t been introduced.
- _Note:_ The security audit relies on Claude’s built-in security knowledge and heuristics. It does not integrate with external scanners, and it cannot automatically distinguish false positives from real issues. All findings should be reviewed by a developer, and addressing or closing any issues remains a manual step for now.

By integrating a security audit into the development cycle, Claude-SDLC helps teams maintain a strong security posture. The AI’s recommendations guide developers in areas they might not be expert in, reducing the likelihood of security incidents in production.

### `/architecture-review` – Architectural Analysis

This command reviews the project’s architecture and design, checking for scalability, maintainability, and adherence to design principles. It’s somewhat meta-level compared to code review, focusing on **how the system is organized** rather than line-by-line code issues.

**Behavior:**
- Claude compiles an understanding of the project’s structure. It looks at the file/project organization (modules, layers), reads any existing architecture documents in `.claude-sdlc/architecture/`, and possibly infers architecture from the code (directory structures, naming conventions, inheritance, etc.).
- It evaluates the architecture against known good practices and the project’s requirements:
    - Are there any obvious bottlenecks or single points of failure in the design?
    - Is the code modular and following separation of concerns, or are there monolithic classes/functions that should be broken down?
    - How does the data flow through the system? Are there clear boundaries (e.g., between frontend, backend, database)? If microservices architecture is expected, is it properly implemented?
    - If an architecture diagram or spec exists, does the current code align with it? (E.g., if the spec says there should be a service layer and repository layer, it checks if those exist and are used appropriately.)
- The output will highlight architecture-level findings:
    - E.g., “The current implementation of authentication is tightly integrated with the user module, which violates the planned modular architecture. Consider extracting authentication logic into its own service to improve reusability.”
    - It might identify areas of high complexity and suggest refactoring into more services or modules.
    - If the project has grown beyond its initial design, it might recommend restructuring (for example, introducing layers, or moving certain files to more appropriate locations).
    - It also praises good architecture: “The use of the observer pattern for the notification module is a strong design choice that ensures decoupling.”
- Importantly, it addresses scalability: if the app usage grows 10x, are there design limitations that would cause issues? Claude might note things like lack of caching, or a need for load balancing, etc., if applicable.
- The recommendations often tie back to the architecture docs. If it finds the docs outdated or incomplete, it suggests updating them (and possibly can update them as part of this command, though typically it just notes it).
- This command can be used during planning (to validate an approach) or periodically to ensure the system’s evolution stays clean.
    

By using `/architecture-review`, teams can get an impartial second opinion on their high-level design. It’s like having an architect review the project on-demand, which is valuable for maintaining a robust system design over time.

### `/fix-issue` – Targeted Issue Resolution

The `/fix-issue` command streamlines the process of diagnosing and fixing a specific issue or bug, especially one tracked in an issue tracker (e.g., GitHub Issues, Linear, etc.). It takes an identifier or description as an argument. For example: `/fix-issue 123` might correspond to “Issue #123” in your tracker, or `/fix-issue login-error` for a described problem.

**Behavior:**
1. **Issue Understanding:** Claude will first interpret the issue context. If an issue ID is provided and internet or API access is available, it could fetch the issue description from the tracker (though by default it may rely on the developer to have included the relevant details in the prompt). It should also use CLI tools such as gh CLI to review issues (the user can supply a GitHub URL for it to use). Otherwise, the argument itself is treated as the description of the problem (e.g. “/fix-issue login-error” where perhaps the developer has the error log open in the editor or has described it).
2. **Reproducing/Identifying Cause:** The command searches the codebase for clues relevant to the issue. This could include:
    - Grepping for error messages or relevant keywords.
    - Running the test suite or a specific failing test if one is associated with the issue.
    - Using any provided stack trace or steps to reproduce (if given in the issue description) to pinpoint the location in code.  
        For example, if the issue is “login page crashes on empty input”, it would inspect the login handler code for null-checks, etc.
3. **Root Cause Analysis:** Claude hypothesizes the root cause of the issue. It may explain in the output what it believes is causing the bug (e.g., “Null pointer exception occurs if the email field is empty, because the code assumes it’s non-empty when trimming the string.”).
4. **Devise a Fix:** The AI then proceeds to modify the code to fix the issue. This is done carefully:
    - It edits the necessary files (the specific function or module) to resolve the bug (e.g., adding a null check or adjusting logic).
    - If applicable, add or update tests to cover this case (ensuring the issue doesn’t regress).
    - It may perform a quick `/build`-like execution of just this fix: compile and run tests relevant to this issue to verify the fix worked.
5. **Confirmation & Documentation:** After applying the fix, Claude marks the issue as resolved in its output. If integrated with a tracker (future enhancement), it could comment on or close the issue with a message. Regardless, it generates a mini-report of what was changed to fix the issue, for the user’s review.
6. The output to the user includes the changed code snippet or summary (“Added a check to return an error if email is empty, and adjusted the UI to display the proper message instead of crashing.”) and notes if all tests now pass.

Using `/fix-issue` essentially automates the debug→fix→test cycle for known bugs. It shines when a failing test or error log is available for the AI to reference. The command ensures a systematic resolution: understand the problem, fix it, and verify the fix (similar to how a developer would). It’s especially powerful when used in conjunction with a bug tracking workflow, making it easy to go from an issue report to a committed patch. _(Note: it does not automatically mark the issue as closed in external trackers—closing or commenting on the issue must be done manually.)_

### `/performance-audit` – Performance Optimization

The `/performance-audit` command examines the codebase for potential performance bottlenecks and inefficiencies. Its purpose is to suggest optimizations that could make the software faster or more resource-efficient.

**Behavior:**
- Claude reviews algorithms and code patterns that are known to be expensive. It focuses on:
    - Hot loops or recursion that might be optimized (e.g., using caching or more efficient algorithms).
    - Database or I/O usage: looks at database queries or file operations in the code to see if they are done in a suboptimal way (such as N+1 query problems, lack of indexing, reading large files into memory fully instead of streaming, etc.).
    - Memory usage patterns: identifies places that might consume a lot of memory (like constructing large in-memory lists) and suggests ways to mitigate (stream processing, lazy evaluation).
    - Frontend performance (if applicable): checks if there are very large bundle sizes, unoptimized assets, or inefficient DOM updates in a web project.
    - Build performance: if the suite includes building processes, it might note if build times are slow due to certain configurations.
- The command might run profiling tools if integrated (though likely it relies on static analysis and heuristics, since dynamic profiling would require running the program under load which is outside scope).
- Output from `/performance-audit` includes:
    - **List of Potential Bottlenecks:** Each item describes the code or component and why it might be slow. E.g., “The function `getAllUsers()` retrieves all user records and then filters in memory; this could be pushing unnecessary load on the application if the user table is large. Consider adding query parameters to fetch only needed records in the database query.”
    - **Severity or Frequency:** It might not quantify precisely, but it can indicate which issues are likely most impactful.
    - **Optimization Suggestions:** For each issue, Claude offers suggestions. This might involve changing an algorithm (like using a set for membership lookup instead of a list), using more efficient library functions, adding caching (e.g., in-memory cache for frequent computations), introducing concurrency or parallelism where appropriate, or adjusting configurations (e.g., enabling gzip compression for responses, etc.).
- If applicable, it references known best practices or standards in performance (like “use pagination for large result sets”, “avoid synchronous waits in event loop”, etc.), which developers can then act on.
- This command doesn’t automatically change code (since performance tuning often requires careful consideration and testing). Instead, it provides a roadmap of areas to optimize. Developers can then decide which optimizations to implement, possibly using Claude Code interactively to make those changes.

By running `/performance-audit`, teams can proactively discover where their app might struggle as usage grows. It’s like having a performance engineer inspect the code. Coupled with real profiling data (not provided by Claude directly), these insights can significantly guide efficient scaling and responsive application behavior.

- _Note:_ The performance audit’s suggestions are based on static analysis and Claude’s internal knowledge. It does not run actual performance measurements, so some identified issues might not be true bottlenecks. Developers should validate recommendations, as the AI cannot automatically filter out all false positives.

## `/generate-tests` – Automated Test Case Generation

The `/generate-tests` command creates test cases to improve coverage and verify the functionality of existing code. Given a target (such as a module, file, or feature name), it generates appropriate unit or integration tests so that the developer can ensure the code behaves as expected.

**Behavior:**
1. **Target Identification:** When invoked with an argument specifying what to test (e.g., a file path, module name, or feature), Claude determines the scope of code to focus on. If no specific target is provided, it may default to generating tests for recently changed code or critical areas of the project (this behavior can be adjusted by context or future settings).
2. **Test Framework Detection:** The command scans the project structure and configuration to detect the testing framework and conventions in use (for example, looking for a `tests/` directory or identifying if the project uses PyTest, Jest, JUnit, etc.). Claude will adopt the appropriate style and framework for the tests. If no testing framework is found, it assumes a default (e.g., PyTest for Python, or a standard xUnit style for other languages) and can create initial testing boilerplate as needed.
3. **Test Case Generation:** Claude analyzes the target code (reading functions, methods, and usage patterns) to determine important behaviors and edge cases. It then writes new test files or adds test functions that cover typical scenarios and edge conditions for that code. For example, if the target is a function, it will create tests for normal inputs, boundary values, and error conditions. If the target is a broader feature, it might generate a suite of tests across multiple modules to exercise that feature end-to-end.
4. **Avoiding Redundancy:** Before creating a test, Claude checks if similar tests already exist (to prevent duplicate coverage). It attempts to complement existing tests: if the project already has some tests for the target, `/generate-tests` will focus on filling gaps in coverage rather than repeating the same assertions.
5. **Optional Execution:** By default, `/generate-tests` only writes the tests. However, if the user runs the command with the `--run-tests` flag (see **Automation Flag** below), Claude will also execute the test suite after generation. This allows it to verify that the new tests run successfully (or to report any failures). Any test failures will be reported in the output, highlighting either issues in the code under test or necessary adjustments to the tests themselves.
6. **Output:** The command outputs a summary in the chat detailing what tests were created (e.g., “Added 5 new test cases covering the UserController and authentication flows”). The actual test code is written to new or existing test files in the repository (for example, in the `tests/` directory or alongside the target code, following project norms). Additionally, a Markdown report (for example, `.claude-sdlc/builds/test-report-<date>.md`) is generated to log the outcomes – listing the new test files and the results of any test run (if performed).

**Constraints & Considerations:**
- `/generate-tests` does not modify application source code – it only creates or updates test files. (If minimal scaffolding is required to enable testing, like adding a testing library dependency or configuration, it will do so, but it won’t alter the logic of the code under test.)
- The quality of generated tests depends on Claude’s understanding of the code. It attempts to infer correct behavior, but developers should review the tests to ensure they assert the right expectations. In cases where the intended behavior isn’t clear from the code or comments, the AI might make assumptions that need adjustment.
- There is no automatic determination of whether a failing generated test indicates a bug in the code; the command’s role is to surface potential issues via tests. It’s up to the developer (or another command like `/fix-issue`) to address any failing tests or adjust the tests if they are too strict or incorrect.

## Security and Testing

Security and testing are first-class considerations in Claude-SDLC. The suite is designed to not only produce code and plans, but also to validate and verify as it goes, reducing the risk of introducing issues and regressions.

**Secure by Design:** The commands incorporate secure coding practices into their workflows. For example, when generating code for a feature, Claude will avoid common vulnerabilities (like it will favor prepared statements for database queries, proper input validation, using established encryption libraries, etc.). The built-in security audit (`/security-audit`) further ensures that the codebase stays secure by catching lapses reddit.com. All commands running code (like `/build` or `/fix-issue`) do so in the isolated context of the developer’s machine and project – they don’t send code outside except via Claude’s own processing. (Claude’s processing, per Anthropic, is done in a privacy-conscious way, but developers should still be mindful not to include secrets in prompts inadvertently. By default, secrets like API keys should be stored in non-scanned config files or environment variables, which the commands will not expose or send.)

**Testing and Verification:** A key advantage of using Claude Code is its ability to run and interpret tests and build output autonomously. The Claude-SDLC commands leverage this to ensure changes are correct:
- The `/build` command, as described, runs compilation and tests at each step (if a project has a test suite or build process). It catches errors immediately. For instance, if adding a new function causes a unit test to fail, Claude will see the test output and adjust accordingly. This approach follows a basic Test-Driven Development (TDD) philosophy implicitly: tasks often include writing tests, and Claude will iterate until tests pass.
- Because Claude can execute shell commands, it can run something like `npm test` or `make test` as part of its script and then parse the results. If tests fail, it logs the failure. This end-to-end loop means that code generated is not only syntactically correct but also functionally verified against existing tests pulsemcp.com.
- The `/code-review` command’s suggestions often include improving test coverage or adding specific test cases for newfound bugs. This encourages developers to continually build out the test suite, which in turn makes the automated build process more robust.
- For safety, any command that modifies code (especially `/build` and `/fix-issue`) works on the user’s local copy of the repository. It may create a separate branch or worktree (this could be an implementation detail: e.g., working on a `feature/<name>` Git branch) to isolate changes until the developer is ready to merge. This way, if something goes wrong, the main branch isn’t broken. The user can review all changes in a diff before merging to main.
- **User oversight:** Despite the automation, human oversight is expected. The commands pause on failures to involve the user, and even on success, it’s recommended that a developer reviews the AI’s output (possibly using `/code-review` itself as a second pair of eyes). The goal is to **augment**, not completely replace, the developer – thereby catching any mistakes the AI might make.

**Security of the Commands:** The commands themselves are Markdown prompts and thus transparent – a developer can inspect them to see what instructions the AI is following. This is important for trust and safety. They define allowed tools in frontmatter (for example, limiting the AI to only run safe shell commands like tests or git, and not arbitrary destructive commands). By default, dangerous operations (like deploying to production or deleting databases) are not included in these workflows, or if they are in future enhancements, they would require explicit user confirmation.
**Testing the Suite:** From a product perspective, Claude-SDLC commands should be tested on various sample projects (different languages, sizes) to ensure they behave as expected. Unit tests for commands would involve running them in a controlled environment and verifying outcomes:
- Does `/create-feature` produce a sensible task list for a given feature prompt?
- Does `/build` actually implement those tasks and catch errors properly?
- Will `/code-review` identify a seeded bug or security flaw in a small sample codebase?
- These could be part of the development of the suite itself (perhaps automated via a CLI that simulates Claude’s behavior, though that’s non-trivial). Regardless, a thorough QA process is implied: because these commands will be trusted with real code, we ensure they have been vetted to not produce destructive or poor-quality output.

In summary, the combination of automated verification (through tests and analysis) and human oversight means that using Claude-SDLC can improve code quality while minimizing risks. It’s like having a safety net: the AI works fast, but the processes in place (tests, reviews, audits) catch issues that arise, leading to a secure and reliable development workflow.

## Automation Flag

The `--auto` flag can be appended to any code-generating command in Claude-SDLC (currently `/build`, `/fix-issue`, `/generate-tests`, and future commands like `/refactor`). This flag automates the final steps of applying changes, allowing the AI to commit or checkpoint its work without user intervention. By default (without `--auto`), these commands will make changes in the workspace for the developer to review and manually commit.
- **Automatic Git Checkpoint:** When `--auto` is used, the AI will automatically save its changes to the repository. Typically, this means creating a new git commit containing all the modifications the command made. In some cases, it could create a separate feature branch or commit with a specific message. This ensures there's a record of the AI-generated changes that can be reviewed or rolled back. The commit (or branch) name is usually derived from the context (for example, a commit like “AI: Implement user-authentication feature” or a branch named `ai-fix-<issue-number>`).
- **Test Run Integration (`--run-tests`):** If the `--auto` flag is combined with a `--run-tests` option, the command will run the project's test suite after making changes (as a sanity check) before finalizing the commit. For instance, `/build <feature> --auto --run-tests` would implement the feature, run the tests, and only then commit the code. If tests pass, the commit is created (and the output will note that tests passed). If tests fail, the behavior may vary by command: generally, Claude will still save the work (perhaps on a separate branch or as a pending commit) but will report the test failures in the output instead of assuming success, so the developer can address them.
- **Output with Commit Info:** When `--auto` is used, the command’s final output (whether in chat or in a Markdown report) will include information about the commit or branch it created. For example, it might say, “✅ Changes have been committed to branch `feature-login-123` (commit `abcd1234`).” This gives the developer a reference to find the code in version control. The commit SHA (identifier) is provided so that one can inspect the exact state or revert it if needed.
- **Non-Mutating Commands:** Commands that do not alter the code (such as `/code-review`, `/architecture-review`, `/performance-audit`, etc.) ignore the `--auto` flag if provided. The flag only affects commands that produce changes to the repository. Using `--auto` on a read-only analysis command has no effect (the command will just run normally without any commit step).

Using the automation flag is especially helpful for speeding up iterative development — for example, automatically saving fixes or new tests that the AI generates. However, even with `--auto`, it's recommended that developers review the AI’s commits (e.g., via a pull request or checking the diff) to ensure the changes meet expectations.

## Future Enhancements
Claude-SDLC is an evolving product. Several enhancements are envisioned to further increase its capabilities and ease of use:
_(Update: The automated test generation feature, originally planned as `/test`, has now been implemented as the `/generate-tests` command.)_
- **Intelligent Command Orchestration:** Currently, developers need to explicitly invoke each command (plan, build, review, etc.). In the future, the system could automatically chain commands based on context. For example, if a user types a high-level request like “Add a new search feature,” the tool could intelligently decide to run the sequence: `/create-feature search`, then `/build search`, then `/code-review`. Essentially, the AI could _choose_ the right workflow or suggest it (as one Reddit user mused, it’d be great if it could figure out which command to run on its own reddit.com). This would make the experience more seamless for users who may not know about each command.
- **Continuous Integration Hooks:** Integrate the command suite with CI/CD pipelines. For instance, a Git hook or CI job could trigger `/code-review` automatically on each pull request, posting the AI’s review comments to the PR. Or a nightly security audit (`/security-audit`) could run and open issues for any new vulnerability found. Anthropic’s introduction of Claude Code _hooks_ (event-driven command triggers) opens up this possibility medium.com. Future versions might include default hooks, such as running `/test` (if a test command is added) on file save or running `/code-review` pre-commit.
- **Quality Gates in CI:** The suite might also incorporate quality gates in CI/CD, such as preventing merges if test coverage falls below a threshold or critical issues are detected. This would enforce automated standards (like a minimum coverage requirement) in the future.
- **Expanded Command Library:** There are many more aspects of SDLC that can be automated. Future commands might include:
    - `/deploy` – to assist with deployment scripts or configurations (with extreme caution and probably only in staging environments initially).
    - `/document` – to create or update documentation for the codebase or API (e.g., update README or create a changelog entry automatically when features are added).
    - `/refactor` – to improve a given part of the codebase (like converting class components to functional components in a React app, or migrating to a new library).
    - `/optimize` – specific targeted optimization command if not covered by performance-audit, possibly even automating some code changes for performance
    - `/database-review` – focusing on database schema and queries (though this could be part of security or performance already).
    - These would expand the suite to cover the full development lifecycle spectrum
- **Add MCP Server support:** Expressly direct the commands to use different MCP (Model, Code, Problem) server functionality that Claude has access to (for advanced use cases or internal tooling integration).
- **Add GitHub Issue integrations:** Instead of creating tasks locally as .md files, the commands could create all of the tasks and subtasks as GitHub issues. This would provide more visibility across the team for who is working on what, and also allow other team members to pick up tasks and finish them within the codebase. (This would require connecting Claude-SDLC with the GitHub API or similar.)
- **Enhanced Multi-Agent Collaboration:** While sub-agents are used internally in `/build`, a future enhancement could allow more explicit multi-agent strategies. For example, one agent could handle writing code while another simultaneously writes tests for a feature, collaborating in real-time. Or agents with different “personas” (one focusing on security, another on performance) could concurrently review the code from different angles, as suggested in emerging patterns linkedin.comyoutube.com. Managing and synchronizing these within a single command run is complex but could greatly speed up processes.
- **User Personalization and Learning:** Over time, the commands could learn project-specific or team-specific preferences. For instance, adjusting how thorough a code review is based on prior feedback (maybe fewer notes on styling if the team is okay with minor deviations, or more focus on certain architectures the team cares about). This could be facilitated by configuration files in `.claude-sdlc/` (e.g., a config that sets the strictness level for reviews, or which categories to emphasize).
- **Better Error Recovery and Explanations:** Although `/build` pauses on errors, future versions could be even more interactive. For example, when pausing, the agent could offer options like “Retry task”, “Skip task”, “Open an interactive debug mode” etc., within the chat. Additionally, integrating with debugging tools (like running the program and observing stack traces live) could be considered.
- **Metrics and Analytics:** The suite could collect metrics on its usage and effectiveness (locally or with user permission). For example, tracking how often a build fails and why, to target improvements in the prompts or logic. Or measuring the time saved by using these commands. Such analytics would help justify the tool’s value to stakeholders and guide future development.
- **Cross-Project Command Sharing:** While we intentionally scoped commands per project for safety and specificity, an org might want a centralized version of Claude-SDLC commands (to enforce certain workflows across projects). A future feature could allow syncing the command files from a central repository, or a versioning system for the command suite itself, so all projects use the latest improvements. This would be more a dev-ops improvement to ease updating the commands.

In summary, the vision is for Claude-SDLC to become an ever more intelligent co-developer, eventually handling not just coding and reviewing, but orchestrating entire development workflows from idea to deployment with minimal intervention. Each enhancement will be carefully evaluated to maintain the balance between autonomy and user control, ensuring developers remain confident and in charge of the development process.

## Conclusion
Claude-SDLC represents a significant step toward an AI-augmented software development lifecycle. By encapsulating best practices into easily invoked slash commands, it allows developers to offload routine and complex tasks alike to a tireless AI assistant. The suite’s design emphasizes **structure, context-awareness, and safety**: every command follows a defined workflow, uses project-specific knowledge (feature plans, architecture docs) for informed reasoning, and includes checkpoints (tests, reviews, user approvals) to keep development on track. Early use of this system has demonstrated faster feature turnaround and more consistent code quality without additional overhead reddit.com.

For product managers, Claude-SDLC offers more predictable development workflows and documentation artifacts (plans, reports) for each feature. For developers, it’s like having an expert pair programmer and reviewer available on demand, speeding up tasks from planning to bug fixes. As we integrate feedback and expand its capabilities, Claude-SDLC aims to evolve into an indispensable toolkit that accelerates development while upholding high standards of quality and security. By maintaining the current project scope for commands and leveraging Claude’s powerful AI model in a controlled manner, we set a foundation where human creativity and AI efficiency work hand-in-hand to deliver better software, faster.

The journey doesn’t end here – as the suite grows (with commands like `/build` now enriching the implementation phase), we’ll continue refining each part of the SDLC that AI can assist with. The future of development is one where mundane or overwhelming tasks are handled by AI, and human developers can focus on creative design, complex problem-solving, and delivering value. Claude-SDLC is a concrete realization of this future, ready to be used and improved in real-world projects starting today.